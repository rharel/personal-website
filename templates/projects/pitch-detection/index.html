{% extends "projects/base/index_base.html" %}

{% from "macros/links.html" import wikilink, githublink, project_page_link %}
{% from "macros/figures.html" import caption %}
{% from "macros/math.html" import include_mathjax %}
{% from "macros/animation_clips.html" import begin_animation_clip, end_animation_clip %}

{% block title %}Pitch Detection{% endblock %}
{% block head %}

    {% set cd = 'projects/pitch-detection/' %}

    {{ super() }}

    {{ include_mathjax() }}

    <link rel="stylesheet" type="text/css"
          href="{{ url_for('static', filename='projects/css/animation_clip.css') }}">
    <link rel="stylesheet" type="text/css"
          href="{{ url_for('static', filename=cd + 'css/pitch_detection_display.css') }}">

    <script src="{{ url_for('static', filename=cd + 'js/music_note_utils.js') }}"></script>
    <script src="{{ url_for('static', filename=cd + 'js/audio.js') }}"></script>
    <script src="{{ url_for('static', filename=cd + 'js/visualization.js') }}"></script>
    <script src="{{ url_for('static', filename=cd + 'js/pitch_detection.js') }}"></script>
    <script src="{{ url_for('static', filename=cd + 'js/figures.js') }}"></script>

{% endblock %}
{% block projectSummary %}

    <p>
        It has been a while since the last time I found myself working on a project
        involving music or even audio in general. In fact, I have another project planned
        revolving around music visualization, but I felt like my memory of the relevant
        topics has somewhat degraded with time. I needed a brief refresher course
        with something simpler to build my confidence back, and this is the principal
        motivation behind this project.
    </p>
    <p>
        When it comes to simple audio processing tasks, I felt like the problem of
        {{ wikilink('pitch', 'Pitch (music)') }} detection has just the right balance
        of difficulty to suit my current goals, which made it an easy choice.
        To summarize: the project's objective is to develop a straightforward
        pitch detection technique and then demonstrate its use in deriving of
        musical notes from captured whistling audio in real time.
    </p>
{% endblock %}
{% block tableOfContents %}

    <h2>Table of contents</h2>

    <ul>
        <li><a href="#section-showcase">Showcase</a></li>
        <li><a href="#section-digital-soundwaves">Digital soundwaves</a></li>
        <li>
            <a href="#section-pitch-detection">Detecting pitch</a>
            <ul>
                <li><a href="#section-frequency-domain">The frequency domain</a></li>
                <li><a href="#section-naive-approach">A naive approach</a></li>
                <li><a href="#section-refined-approach">Refinement</a></li>
            </ul>
        </li>
        <li><a href="#section-source-code">Source code</a></li>
        <li><a href="#section-see-also">See also</a></li>
    </ul>

{% endblock %}
{% block content %}

    {{ super() }}

    <a id="section-showcase"></a>
    <h2>Showcase</h2>

    <p>
        If your browser supports the required capabilities (which it should,
        unless you have neglected to update it for a long while), you should be
        able to view {{ project_page_link('this interactive demo', 'pitch-detection', 'showcase') }}.
    </p>

    <a id="section-digital-soundwaves"></a>
    <h2>Digital soundwaves</h2>

    <p>
        In essence, digital audio is simply a {{ wikilink('discretization') }}
        of measured physical soundwaves into a binary representation in the form
        of a temporal sequence of <b>samples</b>. The samples are captured
        in a regular time interval and each one is a number
        representing the sound's intensity at the time of its capture.
    </p>
    <p>
        The <b>sampling rate</b> is the number of samples we capture per unit time,
        and is usually expressed in {{ wikilink('Hertz') }}.
        The sampling rate is of importance because it may affect the precision
        of any further analysis we wish to apply to the sampled audio.
        The larger the sampling rate, the shorter the temporal gap becomes between
        consecutive samples, and thus the more accurate our approximation of the
        original sound becomes. Figure 1 visualizes the sequence formed when
        sampling a simple {{ wikilink('sine wave') }}.
    </p>
    <div class="figure">
        {{ caption(
            'Figure 1',
            'A sine wave oscillating at 440 Hz.')
        }}
        {{ begin_animation_clip() }}
        <canvas id="figure-1-visual" class="larger"
                width="500" height="150"></canvas>
        {{ end_animation_clip() }}
    </div>
    <p>
        The sine wave has a very neat, smooth form. In practice, the sound waves
        produced by musical instruments go through much more intricate motions than that of
        the sine. Figure 2 visualizes what a piano's middle-C waveform looks like.
    </p>
    <div class="figure">
        {{ caption(
            'Figure 2',
            'A piano\'s middle-C.')
        }}
        {{ begin_animation_clip() }}
        <canvas id="figure-2-visual" class="larger"
                width="500" height="150"></canvas>
        {{ end_animation_clip() }}
    </div>
    <p>
        Of course, we already know that the note played in figure 2 is C4,
        and those of us with musical background could probably identify it as such
        by hearing alone, but how could we extract the correct pitch of notes
        in a piece, given only its sampled waveform (figure 3)?
    </p>
    <div class="figure">
        {{ caption(
            'Figure 3',
            'The scale of C-major played on the piano.')
        }}
        {{ begin_animation_clip() }}
        <canvas id="figure-3-visual" class="larger"
                width="500" height="150"></canvas>
        {{ end_animation_clip() }}
    </div>

    <a id="section-pitch-detection"></a>
    <h2>Detecting pitch</h2>

    <p>
        {{ wikilink('Pitch', 'Pitch (music)') }} is the
        {{ wikilink('perceived frequency', 'Pitch (music)#Pitch_and_frequency') }}
        of sound. Most sounds are not made up of one frequency, but rather
        a sum of many waves at different frequencies and intensitites audible on
        top of each other. Therefore, an initial step towards detecting the pitch of
        a sound is to first sample it, and then extract the frequencies present
        together with their intensities from the sample sequence.
    </p>

    <a id="section-frequency-domain"></a>
    <h3>The frequency domain</h3>

    <p>
        This transformation from the {{ wikilink('time domain') }}
        &mdash; relating time to sound intensity &mdash; to the
        {{ wikilink('frequency domain') }} &mdash; relating a frequency to its
        intensity &mdash; is called the {{ wikilink('Fourier transform') }},
        or in cases where the involved signals are discrete
        (as it is when discussing digital audio), it is rather called the
        {{ wikilink('discrete Fourier transform') }} (or DFT for short).
        Figure 4 shows the DFT of a sound over the course of
        its playback.
    </p>
    <div class="figure">
        {{ caption(
            'Figure 4',
            'A visualization of the frequency domain.
             The horizontal axis denotes frequencies from C3-C6 and the
             intensity of a given frequency is expressed by its corresponding
             bar height.')
        }}
        <div class="subfigure-group">
            <div class="subfigure">
                {{ begin_animation_clip() }}
                <canvas id="figure-4-A-visual" class="larger"
                        width="500" height="150"></canvas>
                {{ end_animation_clip() }}
                <div class="subtitle">
                    A sine wave oscillating at 440 Hz.
                </div>
            </div>
            <div class="subfigure">
                {{ begin_animation_clip() }}
                <canvas id="figure-4-B-visual" class="larger"
                        width="500" height="150"></canvas>
                {{ end_animation_clip() }}
                <div class="subtitle">
                    The scale of C-major played on the piano.
                </div>
            </div>
        </div>
    </div>
    <p>
        With the DFT, we already see the influence the sampling rate has
        on our analysis capabilities: with a sampling rate of \(s\), the maximum
        frequency detectable with the DFT is \(f_{max} = 0.5s\). This frequency
        is called the {{ wikilink('Nyquist frequency') }}. What the DFT does is
        it divides the interval \([0, f_{max})\) into \(m\) bins, each being
        associated with a sub-interval \(B\) and an intensity value \(b\):
    </p>
    <div class="math">
        $$ i \in \{1, 2, \ldots m\} $$
        $$ B_i = [f_{i}, f_{i+1}) \text{ where }
           f_i = \frac{i-1}{m} f_{max} $$
        $$ b_i \in [0, 1] $$
    </div>
    <p>
        Like the sampling rate, the choice of \(m\) also affects our analysis
        capabilities. For example, if \(f_{max} = 100\), and we are interested
        in computing the difference in intensity of frequencies at 10 Hz and 11 Hz,
        we would have to choose \(m >= 100\), because otherwise a single bin would
        include both 10 Hz and 11 Hz in itself, and we would not be able to make a
        distinction between them.
    </p>
    <p>
        For our purposes, we are interested in analyzing frequencies corresponding
        to the notes C2-C6. The frequency of C6 is approximately 1050 Hz. Therefore,
        the usual sampling rate of any standard audio
        (typically 20-100 KHz) is more than enough to cover all
        relevant frequencies. In addition, a bin count \(m\) chosen such that
        \(f_{max} / m <= 4\) would ensure we can distinguish between each individual
        note-pair, as the minimum difference on the C2-C6 range is the one between
        C2 and C#2, which spans approximately 3.9 Hz.
    </p>

    <a id="section-naive-approach"></a>
    <h3>A naive approach</h3>

    <p>
        Assuming we have chosen a sampling rate and a DFT bin count that is
        suitable for our purposes, an initial approach to detecting
        the pitch of a sound at a point in time is to assume that it corresponds to
        the frequency with the greatest intensity at that time. Figure 5 showcases
        a detector using that strategy.
    </p>
    <div class="figure">
        {{ caption(
            'Figure 5',
            'Pitch detection on the sound of the C-major scale being played
             on the piano.')
        }}
        <div class="pitch-detection-display">
            {{ begin_animation_clip() }}
            <canvas id="figure-5-visual" class="larger"
                    width="500" height="150"></canvas>
            {{ end_animation_clip() }}
            <div id="figure-5-detection-result" class="text-display">
                <div class="current-note"></div>
                <div class="log"></div>
            </div>
        </div>
    </div>
    <p>
        The results of this naive approach are not bad, but there is room for
        improvement on two fronts:
    </p>
    <ol>
        <li>
            The detection keeps on going even when the audio is practically
            silent.
        </li>
        <li>
            When there are multiple bins whose high intensities are very
            close to each other, the detection may get jittery as they
            sporadically combat each other for the spot reserved for the bin
            with maximal intensity.
        </li>
    </ol>

    <a id="section-refined-approach"></a>
    <h3>Refinement</h3>

    <p>
        We deal with the first issue by treating all bins whose intensity is smaller
        than some threshold value as having no value at all, i.e. as if \(b_i = 0\)
        instead of its actual (albeit small) value. To overcome jitter, we buffer
        the detection results over some time window, and output only
        the note which appears most often in that buffer.
    </p>
    <p>
        For example: imagine that we perform a note detection at a rate of 100 Hz,
        and that a 6-spot window lists our past six detections as the following:
    </p>
    <div class="math">
        $$ (C4, C4, E5, C4, C4, C4) $$
    </div>
    <p>
        To consider E5 as a plausible pitch would not be a safe bet here. It is more
        likely that the bin corresponding to E5 has just temporarily overtaken
        C4 as the dominant frequency, but only for a brief time. If at a future
        time E5 were to become the actual pitch, our window would eventually
        take a similar form to the following:
    </p>
    <div class="math">
        $$ (C4, C4, E5, E5, E5, E5) $$
    </div>
    <p>
        That is the point in time where it is alright to say that E5 has replaced
        C4 as the dominant pitch.
    </p>
    <p>
        Of course, the chosen window size is important. The rate of detections
        for the animation on this page is roughly dependent on your browser's
        animation frame rates, but if we generally assume that the rate is in the range
        of 30-60 frames per second, than for our purposes a window with 12 spots
        would do moderately well. Figure 6 showcases a detector
        using our refined approach.
    </p>
    <div class="figure">
        {{ caption(
            'Figure 6',
            'Refined pitch detection on the sound of the C-major scale being played
             on the piano.')
        }}
        <div class="pitch-detection-display">
            {{ begin_animation_clip() }}
            <canvas id="figure-6-visual" class="larger"
                    width="500" height="150"></canvas>
            {{ end_animation_clip() }}
            <div id="figure-6-detection-result" class="text-display">
                <div class="current-note"></div>
                <div class="log"></div>
            </div>
        </div>
    </div>
    <p>
        On my machine, figure 6 produces the detected sequence:
    </p>
    <div class="math">
        $$ (C4, G5, D4, E4, F4, G4, A4, B4, C5) $$
    </div>
    <p>
        ...which is not bad at all!
    </p>
    <a id="section-source-code"></a>
    <h2>Source code</h2>

    <p>
        You can view the source code on
        {{ githublink('GitHub', 'rharel', 'js-pitch-detector') }}.
    </p>

    <a id="section-see-also"></a>
    <h2>See also</h2>

    <ul>
        <li>
            Conversion of audio samples to the Fourier domain with the
            {{ wikilink('Fast Fourier Transform') }}.
        </li>
        <li>
            More about
            <a href="https://www.quora.com/Why-are-there-only-12-pitch-notes-C-C-B-in-the-world"
               target="_blank">
                the relationship between musical notes and their frequencies</a>.
        </li>
        <li>
            <a href="https://www.musescore.com/"
               target="_blank">
                MuseScore
            </a>
            is the software I used to generate the synthesized piano audio clips
            on this page.
        </li>
    </ul>

{% endblock %}
{% block lastUpdateStamp %} June, 2017 {% endblock %}
